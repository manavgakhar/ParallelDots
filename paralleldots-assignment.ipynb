{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TobacoShelves Object Detection - PyTorch and Faster RCNN\n--------------------------------\n## ParallelDots coding assignment - Manav Gakhar\n--------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Importing the necessary libraries ","metadata":{}},{"cell_type":"code","source":"import os\nimport collections\nimport pandas as pd\nimport numpy as np\nimport functools\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom sklearn import preprocessing \n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import SequentialSampler","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:42:18.753772Z","iopub.execute_input":"2021-07-04T15:42:18.75415Z","iopub.status.idle":"2021-07-04T15:42:18.762834Z","shell.execute_reply.started":"2021-07-04T15:42:18.754109Z","shell.execute_reply":"2021-07-04T15:42:18.761811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modifying dataframe","metadata":{}},{"cell_type":"code","source":"# path to the folders\nIMG_PATH = \"../input/tobaccoshelves/TobaccoShelves\"\nannotation_path = \"../input/tobaccoshelves/TobaccoShelves_coco_gt_object.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:42:19.686397Z","iopub.execute_input":"2021-07-04T15:42:19.686726Z","iopub.status.idle":"2021-07-04T15:42:19.690523Z","shell.execute_reply.started":"2021-07-04T15:42:19.686678Z","shell.execute_reply":"2021-07-04T15:42:19.689596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modifying dataframe\ndf2 = pd.read_csv(annotation_path)\ndf2['img_path'] = [IMG_PATH + \"/\"+ x for x in df2['image_name']]\ndf2['img_id'] = [x.split(\".\")[0] for x in df2['image_name']]\n\ndf2['labels'] = df2['class']\ndf2['xmin'] = df2['x1']\ndf2['xmax'] = df2['x2']\ndf2['ymin'] = df2['y1']\ndf2['ymax'] = df2['y2']","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:31.412179Z","iopub.execute_input":"2021-07-04T15:47:31.41249Z","iopub.status.idle":"2021-07-04T15:47:31.448422Z","shell.execute_reply.started":"2021-07-04T15:47:31.41246Z","shell.execute_reply":"2021-07-04T15:47:31.447653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining class dictionary manually\nclasses= {0:\"0\", 1:'1',2:'2',3:'3',4:'4',5:'5',6:'6',7:'7',8:'8',9:'9',10:'10',11:'11'}","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:32.615424Z","iopub.execute_input":"2021-07-04T15:47:32.615754Z","iopub.status.idle":"2021-07-04T15:47:32.620163Z","shell.execute_reply.started":"2021-07-04T15:47:32.615722Z","shell.execute_reply":"2021-07-04T15:47:32.619166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-val-test split","metadata":{}},{"cell_type":"code","source":"## Splitting according to img_ids\nimage_ids = df2['img_id'].unique()\ntrain_ids = image_ids[:280]\nvalid_ids = image_ids[-25:]\ntest_ids = image_ids[290:-25]\n\n## Defining dataframes lol\nvalid_df = df2[df2['img_id'].isin(valid_ids)]\ntrain_df = df2[df2['img_id'].isin(train_ids)]\ntest_df = df2[df2['img_id'].isin(test_ids)]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:33.783225Z","iopub.execute_input":"2021-07-04T15:47:33.783575Z","iopub.status.idle":"2021-07-04T15:47:33.804847Z","shell.execute_reply.started":"2021-07-04T15:47:33.783542Z","shell.execute_reply":"2021-07-04T15:47:33.803956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset class and helper functions ","metadata":{}},{"cell_type":"code","source":"# Custom dataset class \nclass TobaccoShelves(Dataset):\n    \n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n        #list of uniques img ids \n        self.image_ids = dataframe['img_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n    \n    def __getitem__(self, index: int):\n        \n        image_id = self.image_ids[index]\n        records = self.df[self.df['img_id'] == image_id]\n        \n        # reading the image using opencv\n        image = cv2.imread(f'{self.image_dir}/{image_id}.JPG', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        rows, cols = image.shape[:2]\n        \n        # getting bbox values\n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        \n        # calc. area of bbox\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # getting value of labels\n        label = records['labels'].values\n        labels = torch.as_tensor(label, dtype=torch.int64)\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        # defining target dictionary with all necessary values\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        # condition for transformations\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1,0)\n            \n            \n            return image, target\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:35.14476Z","iopub.execute_input":"2021-07-04T15:47:35.145081Z","iopub.status.idle":"2021-07-04T15:47:35.157065Z","shell.execute_reply.started":"2021-07-04T15:47:35.145052Z","shell.execute_reply":"2021-07-04T15:47:35.156261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions \n\n# defining albumentations for training and validation sets\ndef get_transform_train():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.2),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format':'pascal_voc', 'label_fields': ['labels']})\n\ndef get_transform_valid():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields':['labels']})\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:35.731041Z","iopub.execute_input":"2021-07-04T15:47:35.731351Z","iopub.status.idle":"2021-07-04T15:47:35.737121Z","shell.execute_reply.started":"2021-07-04T15:47:35.731322Z","shell.execute_reply":"2021-07-04T15:47:35.73622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloaders","metadata":{}},{"cell_type":"code","source":"train_dataset = TobaccoShelves(train_df, IMG_PATH , get_transform_train())\nvalid_dataset = TobaccoShelves(valid_df, IMG_PATH, get_transform_valid())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\n# checking if we have gpu available for training \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:36.761548Z","iopub.execute_input":"2021-07-04T15:47:36.761927Z","iopub.status.idle":"2021-07-04T15:47:36.771085Z","shell.execute_reply.started":"2021-07-04T15:47:36.761895Z","shell.execute_reply":"2021-07-04T15:47:36.770196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model","metadata":{}},{"cell_type":"code","source":"# loading pre-trained model (coco)\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 11  \n\n# no. of input features of classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replacing pre-trained head with new head (for fine-tuning)\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel.to(device)\n\n# defining hyperparameters of model\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:38.445231Z","iopub.execute_input":"2021-07-04T15:47:38.445574Z","iopub.status.idle":"2021-07-04T15:47:39.243815Z","shell.execute_reply.started":"2021-07-04T15:47:38.445544Z","shell.execute_reply":"2021-07-04T15:47:39.242921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting training helpers\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n!git clone https://github.com/pytorch/vision.git\n!cd vision;cp references/detection/utils.py ../;cp references/detection/transforms.py ../;cp references/detection/coco_eval.py ../;cp references/detection/engine.py ../;cp references/detection/coco_utils.py ../","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:46:29.266373Z","iopub.execute_input":"2021-07-04T15:46:29.266636Z","iopub.status.idle":"2021-07-04T15:46:30.578588Z","shell.execute_reply.started":"2021-07-04T15:46:29.266607Z","shell.execute_reply":"2021-07-04T15:46:30.577546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\nimport utils\n# training the model for two epochs\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    \n    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=35)\n    # update the learning rate\n    lr_scheduler.step()\n    \n    # evaluate on the validation dataset\n#     evaluate(model, valid_data_loader, device=device)\n\n#saving the model\ntorch.save(model.state_dict(), 'faster_rcnn_state.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:47:40.641268Z","iopub.execute_input":"2021-07-04T15:47:40.641621Z","iopub.status.idle":"2021-07-04T15:51:10.918885Z","shell.execute_reply.started":"2021-07-04T15:47:40.641589Z","shell.execute_reply":"2021-07-04T15:51:10.917944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"# load  a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n\nWEIGHTS_FILE = \"./faster_rcnn_state.pth\"\n\nnum_classes = 11\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# load the trained weights\nmodel.load_state_dict(torch.load(WEIGHTS_FILE))\n\n# send model to the device\nmodel = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test helper function (for inference)\ndef obj_detector(img):\n    \n    # reafint image using opencv\n    img = cv2.imread(img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n    img /= 255.0\n    img = torch.from_numpy(img)\n    img = img.unsqueeze(0)\n    img = img.permute(0,3,1,2)\n    \n    # defining threshold\n    detection_threshold = 0.1\n    \n    img = list(im.to(device) for im in img)\n    output = model(img)\n    \n    # iterating through list of images\n    for i , im in enumerate(img):\n        \n        # transfering everything to cpu device (if it was on gpu)\n        boxes = output[i]['boxes'].data.cpu().numpy()\n        scores = output[i]['scores'].data.cpu().numpy()\n        labels = output[i]['labels'].data.cpu().numpy()\n        \n        labels = labels[scores >= detection_threshold]\n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    \n    sample = img[0].permute(1,2,0).cpu().numpy()\n    sample = np.array(sample)\n    boxes = output[0]['boxes'].data.cpu().numpy()\n    name = output[0]['labels'].data.cpu().numpy()\n    scores = output[0]['scores'].data.cpu().numpy()\n    boxes = boxes[scores >= detection_threshold].astype(np.int32)\n    names = name.tolist()\n    \n    return names, boxes, sample","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:55:26.897213Z","iopub.execute_input":"2021-07-04T15:55:26.897616Z","iopub.status.idle":"2021-07-04T15:55:26.910716Z","shell.execute_reply.started":"2021-07-04T15:55:26.897572Z","shell.execute_reply":"2021-07-04T15:55:26.909873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plotting inference time images\npred_files = test_df['img_path'].unique()\nplt.figure(figsize=(20,60))\nfor i, images in enumerate(pred_files):\n    if i > 3:break\n    plt.subplot(10,2,i+1)\n    names,boxes,sample = obj_detector(images)\n    for i,box in enumerate(boxes):\n        cv2.rectangle(sample,(box[0], box[1]),(box[2], box[3]),(0, 220, 0), 2) \n\n    plt.axis('off')\n    plt.imshow(sample)\n#     plt.savefig('save_image.png', bbox_inches='tight')  ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T15:55:27.395473Z","iopub.execute_input":"2021-07-04T15:55:27.395805Z","iopub.status.idle":"2021-07-04T15:55:35.656179Z","shell.execute_reply.started":"2021-07-04T15:55:27.395774Z","shell.execute_reply":"2021-07-04T15:55:35.655439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}